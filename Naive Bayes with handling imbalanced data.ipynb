{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nimport warnings\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nsbn.set(style='white')\nwarnings.filterwarnings('ignore')","execution_count":72,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef lemmatize(sentence):\n    words = sentence.split()\n    sentence = \"\"\n    for word in words:\n        sentence += lemmatizer.lemmatize(word) + \" \"\n    sentence.strip()\n    return sentence\n\n\ndef handle_contractions(sentence):\n    cList = {\"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it had\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'alls\": \"you alls\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you had\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you you will\",\n    \"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n    }\n    for word, replacement in cList.items():\n        sentence = sentence.replace(word, replacement)\n    return sentence\n\n\ndef handle_punctuations(sentence):\n    for punct in '?!.,#\"$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’' + \"&/-'\":\n        sentence = sentence.replace(punct, '')\n    return sentence\n\n\ndef handle_stopwords(sentence):\n    words = sentence.split()\n    sentence = \"\"\n    for word in words:\n        if word not in stop_words:\n            sentence += word + \" \"\n    sentence.strip()\n    return sentence\n\n\ntrain['text'] = list(map(handle_contractions, tqdm(train['text'].tolist())))\ntrain['text'] = list(map(handle_punctuations, tqdm(train['text'].tolist())))\ntrain['text'] = list(map(handle_stopwords, tqdm(train['text'].tolist())))\ntrain['text'] = list(map(lemmatize, tqdm(train['text'].tolist())))\n\ntest['text'] = list(map(handle_contractions, tqdm(test['text'].tolist())))\ntest['text'] = list(map(handle_punctuations, tqdm(test['text'].tolist())))\ntest['text'] = list(map(handle_stopwords, tqdm(test['text'].tolist())))\ntest['text'] = list(map(lemmatize, tqdm(test['text'].tolist())))","execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=5279), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c17e8302924c19952c7db77f72aeaf"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=5279), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d125df3493a4e79a2f9fb1380f9ecfb"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=5279), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"258c5786626b4f73bb691f42c36d7237"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=5279), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ac115e7e5c48d591c6799e8885864f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=2924), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a433b5082c504d84883234e4dea28465"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=2924), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5b275093614791a7e4708a9c0f023d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=2924), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9092e364993841e19ba7ada85144fefc"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=2924), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd08e47615de4e1d824ad5004542fff1"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'].value_counts()","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"2    3825\n1     837\n0     617\nName: sentiment, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_0 = train.loc[train['sentiment']==0]\ntrain_1 = train.loc[train['sentiment']==1].sample(n=617,random_state=42)\ntrain_2 = train.loc[train['sentiment']==2].sample(n=617,random_state=42)\ntrain = pd.concat([train_0, train_1, train_2])\ntrain['sentiment'].value_counts()","execution_count":65,"outputs":[{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"2    617\n1    617\n0    617\nName: sentiment, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train['text']\ny = train['sentiment']\nx_pred = test['text']\n\nvector = CountVectorizer().fit(x)\nx = vector.transform(x)\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n\nmodel = MultinomialNB()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nscore = f1_score(y_val, y_pred, average='micro')\nprint(score)","execution_count":79,"outputs":[{"output_type":"stream","text":"0.4582210242587601\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = train['text']\n# y = train['sentiment']\n# x_pred = test['text']\n\n# vector = CountVectorizer().fit(x)\n# x = vector.transform(x)\n# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# model = GaussianNB()\n# model.fit(x_train.toarray(), y_train)\n# y_pred = model.predict(x_val.toarray())\n# score = f1_score(y_val, y_pred, average='micro')\n# print(score)","execution_count":68,"outputs":[{"output_type":"stream","text":"0.38005390835579517\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = train['text']\n# y = train['sentiment']\n# x_pred = test['text']\n\n# vector = CountVectorizer().fit(x)\n# x = vector.transform(x)\n# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# model = ComplementNB()\n# model.fit(x_train, y_train)\n# y_pred = model.predict(x_val)\n# score = f1_score(y_val, y_pred, average='micro')\n# print(score)","execution_count":69,"outputs":[{"output_type":"stream","text":"0.4555256064690027\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = train['text']\n# y = train['sentiment']\n# x_pred = test['text']\n\n# vector = CountVectorizer().fit(x)\n# x = vector.transform(x)\n# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# model = BernoulliNB()\n# model.fit(x_train, y_train)\n# y_pred = model.predict(x_val)\n# score = f1_score(y_val, y_pred, average='micro')\n# print(score)","execution_count":70,"outputs":[{"output_type":"stream","text":"0.40970350404312667\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = train['text']\n# y = train['sentiment']\n# x_pred = test['text']\n\n# vector = CountVectorizer().fit(x)\n# x = vector.transform(x)\n# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# model = KNeighborsClassifier(n_neighbors=3)\n# model.fit(x_train, y_train)\n# y_pred = model.predict(x_val)\n# score = f1_score(y_val, y_pred, average='micro')\n# print(score)","execution_count":75,"outputs":[{"output_type":"stream","text":"0.3773584905660377\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_val, y_pred)","execution_count":71,"outputs":[{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"array([[30, 88, 20],\n       [ 5, 99,  6],\n       [20, 80, 23]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultinomialNB().fit(x, y)\nx_pred = vector.transform(x_pred)\ny_pred = model.predict(x_pred)\n\nsample = pd.read_csv('../input/sample.csv')\nsample['sentiment'] = y_pred\nsample.to_csv('BernoulliNB-with-Preprocessing.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}